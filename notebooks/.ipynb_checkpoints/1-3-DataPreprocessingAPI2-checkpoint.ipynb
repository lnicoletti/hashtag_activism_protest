{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import zipfile\n",
    "from os.path import isfile, join\n",
    "from random import sample \n",
    "import ast\n",
    "from json.decoder import JSONDecodeError\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from functools import reduce\n",
    "import warnings\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# which hexagon resolution was the data collected in?\n",
    "res = \"4res\"\n",
    "\n",
    "directory = os.chdir(f'../../PoliceBrutality/data/raw/full_archive/{res}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_files[300]['data'][0]['referenced_tweets_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_files[300]['includes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b8f0fcd4e849adbf0adb82257e707e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='user and place assignment'), FloatProgress(value=0.0, max=7965.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "files = [file for file in os.listdir(directory) if file.endswith('.json')]\n",
    "hex_ids = [file.split('_')[1] for file in files]\n",
    "tweet_files = [json.loads(line) for file in files for line in open(file, 'r')]\n",
    "\n",
    "# add user and place info to each tweet\n",
    "for tweet_file in tqdm(tweet_files, desc = \"user and place assignment\"):\n",
    "    for i in range(len(tweet_file['data'])):\n",
    "        for j in range(len(tweet_file['includes']['users'])):\n",
    "            try:\n",
    "                \n",
    "                if tweet_file['data'][i]['author_id'] == tweet_file['includes']['users'][j]['id']:\n",
    "                    tweet_file['data'][i]['user_info'] = tweet_file['includes']['users'][j]\n",
    "                    \n",
    "                if 'in_reply_to_user_id' in tweet_file['data'][i]:\n",
    "                        if tweet_file['data'][i]['in_reply_to_user_id'] == tweet_file['includes']['users'][j]['id']:\n",
    "                            tweet_file['data'][i]['in_reply_to_user_info'] = tweet_file['includes']['users'][j]\n",
    "                        \n",
    "                for k in range(len(tweet_file['includes']['places'])):\n",
    "\n",
    "                    if 'geo' in tweet_file['data'][i]:\n",
    "                        if 'place_id' in tweet_file['data'][i]['geo']:\n",
    "                            if tweet_file['data'][i]['geo']['place_id'] == tweet_file['includes']['places'][k]['id']:\n",
    "                                tweet_file['data'][i]['place_info'] = tweet_file['includes']['places'][k]\n",
    "                            \n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "            if \"referenced_tweets\" in tweet_file['data'][i]:\n",
    "                if \"tweets\" in tweet_file['includes']:\n",
    "                    for k in range(len(tweet_file['includes']['tweets'])):\n",
    "                        if tweet_file['data'][i]['referenced_tweets'][0]['id'] == tweet_file['includes']['tweets'][k]['id']:\n",
    "                            tweet_file['data'][i]['referenced_tweets_info'] = tweet_file['includes']['tweets'][k]\n",
    "                \n",
    "\n",
    "print(\"Done with user and place objects...\")\n",
    "# assign hexagon id to each tweet\n",
    "for tweet_file in tqdm(tweet_files, desc=\"hex id assignment\"):\n",
    "    for hex_id in hex_ids:\n",
    "        for i in range(len(tweet_file['data'])):\n",
    "            tweet_file['data'][i]['hex_id'] = hex_id\n",
    "            \n",
    "print(\"Done with hex ids...\")\n",
    "\n",
    "tweets = [tweet_file['data'][i] for tweet_file in tweet_files for i in range(len(tweet_file['data']))]\n",
    "\n",
    "print(\"Loaded a sample of\", len(tweet_files), \"files and\", len(tweets), \"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## info\n",
    "tweet_id = []\n",
    "conversation_id = []\n",
    "created_at = []\n",
    "text = []\n",
    "coordinates = []\n",
    "# hashtags\n",
    "hashtags = []\n",
    "\n",
    "## place\n",
    "place_full_name = []\n",
    "place_country_code = []\n",
    "place_country = []\n",
    "place_type = []\n",
    "place_bbox = []\n",
    "place_from_text = []\n",
    "\n",
    "## Public metrics\n",
    "retweet_count = []\n",
    "reply_count = []\n",
    "like_count = []\n",
    "quote_count = []\n",
    "\n",
    "# user info\n",
    "user_id = []\n",
    "username = []\n",
    "user_full_name = []\n",
    "user_following = []\n",
    "user_followers = []\n",
    "user_location = []\n",
    "user_created_at = []\n",
    "user_bio = []\n",
    "\n",
    "# in reply to user info\n",
    "in_reply_to_user_id = []\n",
    "in_reply_to_username = []\n",
    "in_reply_to_user_full_name = []\n",
    "in_reply_to_user_following = []\n",
    "in_reply_to_user_followers = []\n",
    "in_reply_to_user_location = []\n",
    "in_reply_to_user_created_at = []\n",
    "in_reply_to_user_bio = []\n",
    "\n",
    "# referenced tweets info\n",
    "referenced_tweet_id = []\n",
    "referenced_tweet_type = []\n",
    "referenced_tweet_text = []\n",
    "referenced_tweet_created_at = []\n",
    "referenced_tweet_author_id = []\n",
    "referenced_tweet_conversation_id = []\n",
    "\n",
    "# hexagon_id info\n",
    "hex_ids = []\n",
    "\n",
    "for tweet in tqdm(tweets, desc = \"data preprocessing\"):\n",
    "    \n",
    "    '''tweet info'''\n",
    "    # tweet id\n",
    "    tweet_id.append(tweet['id'])\n",
    "    # tweet text\n",
    "    text.append(tweet['text'])\n",
    "    # created_at\n",
    "    created_at.append(tweet['created_at'])\n",
    "    # conversation_id\n",
    "    conversation_id.append(tweet['conversation_id'])\n",
    "    # retweet_count\n",
    "    retweet_count.append(tweet['public_metrics']['retweet_count'])\n",
    "    # retweet_count\n",
    "    quote_count.append(tweet['public_metrics']['quote_count'])\n",
    "    # retweet_count\n",
    "    like_count.append(tweet['public_metrics']['like_count'])\n",
    "    # retweet_count\n",
    "    reply_count.append(tweet['public_metrics']['reply_count'])\n",
    "    # hashtags\n",
    "    tags = []\n",
    "    try:\n",
    "        for hashtag in tweet['entities']['hashtags']:\n",
    "            tags.append(hashtag['tag'])\n",
    "        hashtags.append(tags)\n",
    "    except KeyError:\n",
    "        hashtags.append(np.NaN)\n",
    "    \n",
    "    '''tweet place info'''\n",
    "    # coordinates\n",
    "    try:\n",
    "        coordinates.append(tweet['geo']['coordinates']['coordinates'])\n",
    "    except KeyError:\n",
    "        coordinates.append(np.NaN)\n",
    "    # place_full_name\n",
    "    try:\n",
    "        place_full_name.append(tweet['place_info']['full_name'])\n",
    "    except KeyError:\n",
    "        place_full_name.append(np.NaN)\n",
    "    # place_country_code \n",
    "    try:\n",
    "        place_country_code.append(tweet['place_info']['country_code'])\n",
    "    except KeyError:\n",
    "        place_country_code.append(np.NaN)\n",
    "    # place_type\n",
    "    try:\n",
    "        place_type.append(tweet['place_info']['place_type'])\n",
    "    except KeyError:\n",
    "        place_type.append(np.NaN)\n",
    "    # place_country\n",
    "    try:\n",
    "        place_country.append(tweet['place_info']['country'])\n",
    "    except KeyError:\n",
    "        place_country.append(np.NaN)\n",
    "    # place_bbox\n",
    "    try:\n",
    "        place_bbox.append(tweet['place_info']['geo']['bbox'])\n",
    "    except KeyError:\n",
    "        place_bbox.append(np.NaN)\n",
    "    # place_from_text\n",
    "    text_places = []\n",
    "    try:\n",
    "        for item in tweet['entities']['annotations']:\n",
    "            if item['type'] == 'Place':\n",
    "                text_places.append(item['normalized_text'])\n",
    "            else:\n",
    "                text_places.append(np.NaN)\n",
    "        place_from_text.append(text_places)\n",
    "    except KeyError:\n",
    "        place_from_text.append(np.NaN)\n",
    "    \n",
    "    '''user info'''\n",
    "    if 'user_info' in tweet.keys():\n",
    "        # get user id\n",
    "        user_id.append(tweet['user_info']['id'])\n",
    "        # get user username\n",
    "        username.append(tweet['user_info']['username'])\n",
    "        # get user user_full_name\n",
    "        user_full_name.append(tweet['user_info']['name'])\n",
    "        # get user friends\n",
    "        user_following.append(tweet['user_info']['public_metrics']['following_count'])\n",
    "        # get user followers\n",
    "        user_followers.append(tweet['user_info']['public_metrics']['followers_count'])\n",
    "        # get user_location\n",
    "        try:\n",
    "            user_location.append(tweet['user_info']['location'])\n",
    "        except KeyError:\n",
    "            user_location.append(np.NaN)\n",
    "        # get user_created_at\n",
    "        user_created_at.append(tweet['user_info']['created_at'])\n",
    "        # get user bio\n",
    "        user_bio.append(tweet['user_info']['description'])\n",
    "    else:\n",
    "        user_id.append(np.NaN)\n",
    "        username.append(np.NaN)\n",
    "        user_full_name.append(np.NaN)\n",
    "        user_following.append(np.NaN)\n",
    "        user_followers.append(np.NaN)\n",
    "        user_location.append(np.NaN)\n",
    "        user_created_at.append(np.NaN)\n",
    "        user_bio.append(np.NaN)\n",
    "        \n",
    "    '''in reply to user info'''\n",
    "    if 'in_reply_to_user_info' in tweet.keys():\n",
    "        # get user id\n",
    "        in_reply_to_user_id.append(tweet['in_reply_to_user_info']['id'])\n",
    "        # get user username\n",
    "        in_reply_to_username.append(tweet['in_reply_to_user_info']['username'])\n",
    "        # get user user_full_name\n",
    "        in_reply_to_user_full_name.append(tweet['in_reply_to_user_info']['name'])\n",
    "        # get user friends\n",
    "        in_reply_to_user_following.append(tweet['in_reply_to_user_info']['public_metrics']['following_count'])\n",
    "        # get user followers\n",
    "        in_reply_to_user_followers.append(tweet['in_reply_to_user_info']['public_metrics']['followers_count'])\n",
    "        # get user_location\n",
    "        try:\n",
    "            in_reply_to_user_location.append(tweet['in_reply_to_user_info']['location'])\n",
    "        except KeyError:\n",
    "            in_reply_to_user_location.append(np.NaN)\n",
    "        # get user_created_at\n",
    "        in_reply_to_user_created_at.append(tweet['in_reply_to_user_info']['created_at'])\n",
    "        # get user bio\n",
    "        in_reply_to_user_bio.append(tweet['in_reply_to_user_info']['description'])\n",
    "    else:\n",
    "        in_reply_to_user_id.append(np.NaN)\n",
    "        in_reply_to_username.append(np.NaN)\n",
    "        in_reply_to_user_full_name.append(np.NaN)\n",
    "        in_reply_to_user_following.append(np.NaN)\n",
    "        in_reply_to_user_followers.append(np.NaN)\n",
    "        in_reply_to_user_location.append(np.NaN)\n",
    "        in_reply_to_user_created_at.append(np.NaN)\n",
    "        in_reply_to_user_bio.append(np.NaN)\n",
    "        \n",
    "        \n",
    "    '''in reply to user info'''\n",
    "    if 'referenced_tweets_info' in tweet.keys():\n",
    "        # get tweet id\n",
    "        referenced_tweet_id.append(tweet['referenced_tweets'][0]['id'])\n",
    "        # get tweet type\n",
    "        referenced_tweet_type.append(tweet['referenced_tweets'][0]['type'])\n",
    "        # get tweet text\n",
    "        referenced_tweet_text.append(tweet['referenced_tweets_info']['text'])\n",
    "        # get tweet created_at\n",
    "        referenced_tweet_created_at.append(tweet['referenced_tweets_info']['created_at'])\n",
    "        # get tweet author id\n",
    "        referenced_tweet_author_id.append(tweet['referenced_tweets_info']['author_id'])\n",
    "        # get tweet conversation id\n",
    "        referenced_tweet_conversation_id.append(tweet['referenced_tweets_info']['conversation_id'])\n",
    "    else:\n",
    "        referenced_tweet_id.append(np.NaN)\n",
    "        referenced_tweet_type.append(np.NaN)\n",
    "        referenced_tweet_text.append(np.NaN)\n",
    "        referenced_tweet_created_at.append(np.NaN)\n",
    "        referenced_tweet_author_id.append(np.NaN)\n",
    "        referenced_tweet_conversation_id.append(np.NaN)\n",
    "\n",
    "    '''hexagon collection info'''\n",
    "    hex_ids.append(tweet['hex_id'])\n",
    "    \n",
    "# df columns\n",
    "cols = ['tweet_id', 'conversation_id', 'created_at', 'text', 'coordinates', 'hashtags', 'place_full_name','place_country_code','place_country',\n",
    "        'place_type','place_bbox', 'place_from_text', 'retweet_count', 'reply_count', 'like_count', 'quote_count', 'user_id', \n",
    "        'username', 'user_full_name', 'user_following', 'user_followers', 'user_location', 'user_created_at', 'user_bio', \n",
    "        'in_reply_to_user_id', 'in_reply_to_username', 'in_reply_to_user_full_name', 'in_reply_to_user_following', \n",
    "        'in_reply_to_user_followers', 'in_reply_to_user_location', 'in_reply_to_user_created_at', 'in_reply_to_user_bio', \n",
    "        'referenced_tweet_id', 'referenced_tweet_type', 'referenced_tweet_text', 'referenced_tweet_created_at', \n",
    "        'referenced_tweet_author_id', 'referenced_tweet_conversation_id', 'hex_id']\n",
    "\n",
    "tweets_df = pd.DataFrame(list(zip(tweet_id,conversation_id,created_at,text,coordinates,hashtags,place_full_name,\n",
    "                                  place_country_code,place_country,place_type,place_bbox,place_from_text,retweet_count, \n",
    "                                  reply_count,like_count,quote_count,user_id,username,user_full_name,user_following, \n",
    "                                  user_followers,user_location,user_created_at,user_bio,in_reply_to_user_id,\n",
    "                                  in_reply_to_username,in_reply_to_user_full_name, in_reply_to_user_following,\n",
    "                                  in_reply_to_user_followers,in_reply_to_user_location, in_reply_to_user_created_at, \n",
    "                                  in_reply_to_user_bio, referenced_tweet_id, referenced_tweet_type, referenced_tweet_text, \n",
    "                                  referenced_tweet_created_at, referenced_tweet_author_id, referenced_tweet_conversation_id,\n",
    "                                  hex_ids)), columns = cols)\n",
    "\n",
    "\n",
    "print(\"Processed\", len(tweets_df), \"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from shapely.geometry import box\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# get bbox centroid for each \"place\" that is a polygon\n",
    "tweets_df['coords_from_place'] = np.NaN\n",
    "tweets_df['coords_from_place'][tweets_df.place_bbox.isna()==False] = tweets_df.place_bbox[tweets_df.place_bbox.isna()==False].apply(lambda x: box(*x).centroid)\n",
    "tweets_df['coordinates'][tweets_df.coordinates.isna()==False] = tweets_df.coordinates[tweets_df.coordinates.isna()==False].apply(lambda x: Point(x))\n",
    "\n",
    "tweets_df['coords_comb'] = np.NaN\n",
    "tweets_df['coords_comb'][tweets_df.coordinates.isna()==False] = tweets_df['coordinates'][tweets_df.coordinates.isna()==False]\n",
    "tweets_df['coords_comb'][tweets_df['coordinates'].isna()==True] = tweets_df['coords_from_place'][tweets_df['coordinates'].isna()==True]\n",
    "\n",
    "# (tweets_df['place_type'].isin(['poi', 'neighborhood', 'admin']))&(\n",
    "\n",
    "# tweets_df['coordinates']\n",
    "\n",
    "# [(tweets_df['place_type'].isin(['poi', 'neighborhood', 'admin']))&(tweets_df['coordinates'].isna()==True)] = tweets_df['coords_from_place']\n",
    "\n",
    "# tweets_df.place_bbox.apply(lambda x: box(*x)).centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv(f'../../processed/tweets_df_fs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(f'../../processed/tweets_df_fs.csv').drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = 6\n",
    "# tweets_df = pd.read_csv(f'../../data/processed/tweets_df_{res}res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['place_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", len(tweets_df[tweets_df.coordinates.isna()==False]), \"tweets with lon-lat coordinates,\", \n",
    "                   len(tweets_df[(tweets_df.coordinates.isna()==False)|(tweets_df.place_type != \"city\")]),\n",
    "                   \"tweets with either lon-lat coordinates or location information at sub city level, and\",\n",
    "                   len(tweets_df), \"tweets with any location information.\")\n",
    "print(\"---------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"There are\", len(tweets_df.user_id.unique()), \"unique users, and\", len(tweets_df.conversation_id.unique()), \"unique conversations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_gdf = gpd.GeoDataFrame(tweets_df, geometry = tweets_df['coords_comb'], crs = \"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_gdf.drop(['coords_from_place', 'coordinates'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_gdf[['text','place_full_name', 'geometry']].to_file(f'../../processed/tweets_gdf_fs.geojson', driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_gdf.coordinates[tweets_gdf.coordinates.isna()==False].x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
