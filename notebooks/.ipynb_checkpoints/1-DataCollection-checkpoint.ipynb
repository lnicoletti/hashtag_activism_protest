{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twarc import Twarc\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\Leonardo\\OneDrive\\Documents\\TU_Delft\\CodingProjects\\PoliceBrutality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "t = Twarc(app_auth=True)\n",
    "\n",
    "keywords = 'racism OR george floyd OR police brutality OR black lives matter OR killing black OR blacklivesmatter OR police violence OR ahmaud arbery OR #blacklivesmatter OR #blm OR #JusticeForGeorgeFloyd OR #ICantBreathe OR #GeorgeFloyd -filter:retweets'\n",
    "\n",
    "with open('data/raw/tweet_data3.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for tweet in t.search(keywords):\n",
    "        json.dump(tweet, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'racism OR george floyd OR police brutality OR black lives matter OR killing black OR blacklivesmatter OR police violence OR ahmaud arbery OR #blacklivesmatter OR #blm OR #JusticeForGeorgeFloyd OR #ICantBreathe OR #GeorgeFloyd -filter:retweets' > tweets_data4.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "t = Twarc(app_auth=True)\n",
    "\n",
    "keywords = 'racism OR george floyd OR police brutality OR black lives matter OR killing black OR blacklivesmatter OR police violence OR #blacklivesmatter OR #blm OR #JusticeForGeorgeFloyd OR #ICantBreathe OR #GeorgeFloyd'\n",
    "\n",
    "with open('data/raw/tweet_data_rt.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for tweet in t.filter(keywords):\n",
    "        json.dump(tweet, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('data/raw/tweet_data2.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leonardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Leonardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "#Twitter credentials for the app\n",
    "consumer_key = 'uFZGUI4934oUJ6OzOrneWtiva'\n",
    "consumer_secret = 'Zm7B75DdwBDx5rLCMhLChQXhWEEfhqKf00gJFXYL7crFoDJi9A'\n",
    "access_key= '1200171740262998017-2sofVaYjzxkX4ZHjCLjXMezWrK5vZd'\n",
    "access_secret = 'izbr2xHkbncDqxg17Fmimt45KqKrdCRKYSPsFASSEEknF'\n",
    "\n",
    "#pass twitter credentials to tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#file location changed to \"data/telemedicine_data_extraction/\" for clearer path\n",
    "police_brutality_tweets = \"data/raw/tweet_data.csv\"\n",
    "\n",
    "#columns of the csv file\n",
    "COLS = ['id', 'created_at', 'source', 'original_text','clean_text', 'sentiment','polarity','subjectivity', 'lang',\n",
    "        'favorite_count', 'retweet_count', 'original_author', 'possibly_sensitive', 'hashtags',\n",
    "        'user_mentions', 'place', 'place_coord_boundaries']\n",
    "\n",
    "#set two date variables for date range\n",
    "# start_date = '2020-05-22'\n",
    "start_date = '2018-05-22'\n",
    "\n",
    "# end_date = '2018-10-31'\n",
    "\n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "\n",
    "#mrhod clean_tweets()\n",
    "def clean_tweets(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "\n",
    "    #after tweepy preprocessing the colon left remain after removing mentions\n",
    "    #or RT sign in the beginning of the tweet\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "\n",
    "\n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "\n",
    "    #looping through conditions\n",
    "    for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)\n",
    "\n",
    "#method write_tweets()\n",
    "def write_tweets(keyword, file):\n",
    "    # If the file exists, then read the existing data from the CSV file.\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, header=0)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=COLS)\n",
    "    #page attribute in tweepy.cursor and iteration\n",
    "    for page in tweepy.Cursor(api.search, q=keyword,\n",
    "                              count=200, include_rts=False, since=start_date, tweet_mode='extended').pages(10):\n",
    "        for status in page:\n",
    "            new_entry = []\n",
    "            status = status._json\n",
    "\n",
    "            ## check whether the tweet is in english or skip to the next tweet\n",
    "            if status['lang'] != 'en':\n",
    "                continue\n",
    "\n",
    "            #when run the code, below code replaces the retweet amount and\n",
    "            #no of favorires that are changed since last download.\n",
    "            if status['created_at'] in df['created_at'].values:\n",
    "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n",
    "                   status['retweet_count'] != df.at[i, 'retweet_count']:\n",
    "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                continue\n",
    "\n",
    "\n",
    "            #tweepy preprocessing called for basic preprocessing\n",
    "            clean_text = p.clean(status['full_text'])\n",
    "\n",
    "            #call clean_tweet method for extra preprocessing\n",
    "            filtered_tweet=clean_tweets(clean_text)\n",
    "\n",
    "            #pass textBlob method for sentiment calculations\n",
    "            blob = TextBlob(filtered_tweet)\n",
    "            Sentiment = blob.sentiment\n",
    "\n",
    "            #seperate polarity and subjectivity in to two variables\n",
    "            polarity = Sentiment.polarity\n",
    "            subjectivity = Sentiment.subjectivity\n",
    "\n",
    "            #new entry append\n",
    "            new_entry += [status['id'], status['created_at'],\n",
    "                          status['source'], status['full_text'],filtered_tweet, Sentiment,polarity,subjectivity, status['lang'],\n",
    "                          status['favorite_count'], status['retweet_count']]\n",
    "\n",
    "            #to append original author of the tweet\n",
    "            new_entry.append(status['user']['screen_name'])\n",
    "\n",
    "            try:\n",
    "                is_sensitive = status['possibly_sensitive']\n",
    "            except KeyError:\n",
    "                is_sensitive = None\n",
    "            new_entry.append(is_sensitive)\n",
    "\n",
    "            # hashtags and mentions are saved using comma separted\n",
    "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "            new_entry.append(hashtags)\n",
    "            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "            new_entry.append(mentions)\n",
    "\n",
    "            #get location of the tweet if possible\n",
    "            try:\n",
    "                location = status['user']['location']\n",
    "            except TypeError:\n",
    "                location = ''\n",
    "            new_entry.append(location)\n",
    "\n",
    "            try:\n",
    "                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n",
    "            except TypeError:\n",
    "                coordinates = None\n",
    "            new_entry.append(coordinates)\n",
    "\n",
    "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
    "            df = df.append(single_tweet_df, ignore_index=True)\n",
    "            csvFile = open(file, 'a' ,encoding='utf-8')\n",
    "    df.to_csv(csvFile, mode='a', columns=COLS, index=False, encoding=\"utf-8\")\n",
    "\n",
    "#declare keywords as a query for three categories\n",
    "police_brutality_keywords = 'racism OR george floyd OR police brutality OR black lives matter OR killing black OR blacklivesmatter OR police violence OR ahmaud arbery OR #blacklivesmatter OR #blm OR #JusticeForGeorgeFloyd OR #ICantBreathe OR #GeorgeFloyd -filter:retweets'\n",
    "\n",
    "#call main method passing keywords and file path\n",
    "write_tweets(police_brutality_keywords,  police_brutality_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener \n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    def __init__(self, api, file_name_base):\n",
    "        self.tweet_list=[]\n",
    "        self.count = 1\n",
    "        self.file_name_base = file_name_base\n",
    "       \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            j = json.loads(data)\n",
    "            self.tweet_list.append(j)\n",
    "            n = len(self.tweet_list)\n",
    "  \n",
    "            if (n % 100 == 0):\n",
    "                ts = time.time()\n",
    "                stamp = datetime.datetime.fromtimestamp(ts).strftime('%m%d%H%M%S')\n",
    "                file_name = self.file_name_base + stamp + '.json'\n",
    "                with open(file_name , 'w') as f:\n",
    "                    json.dump(self.tweet_list, f)\n",
    "#                     f.write('\\n')\n",
    "                print(\"Output File\", self.count)\n",
    "                self.count = self.count + 1\n",
    "                self.tweet_list = []\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data:\", str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(\"Error\",status)\n",
    "        return False\n",
    "\n",
    "class Credentials:\n",
    "    def __init__(self, access_token, access_token_secret, consumer_key, \n",
    "        consumer_secret):\n",
    "        self.access_token = access_token\n",
    "        self.access_token_secret = access_token_secret\n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "\n",
    "\n",
    "def mine_tweets(credentials, search_terms, file_name):\n",
    "    auth = tweepy.OAuthHandler(credentials.consumer_key, credentials.consumer_secret)\n",
    "    auth.set_access_token(credentials.access_token, credentials.access_token_secret)\n",
    "\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    twitter_stream = Stream(auth, MyListener(auth, file_name))\n",
    "\n",
    "    print(\"Starting up!\")\n",
    "\n",
    "    while(True):\n",
    "        try:\n",
    "            twitter_stream.filter(track=search_terms)\n",
    "        except:\n",
    "            time.sleep(30)\n",
    "            print(\"program restart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up!\n",
      "Output File 1\n",
      "Output File 2\n",
      "Output File 3\n",
      "Output File 4\n",
      "Output File 5\n"
     ]
    }
   ],
   "source": [
    "import tweepy \n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener \n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "# from twitter_miner import *\n",
    "\n",
    "ACCESS_TOKEN = '1200171740262998017-2sofVaYjzxkX4ZHjCLjXMezWrK5vZd'\n",
    "ACCESS_TOKEN_SECRET = 'izbr2xHkbncDqxg17Fmimt45KqKrdCRKYSPsFASSEEknF'\n",
    "CONSUMER_KEY = 'uFZGUI4934oUJ6OzOrneWtiva'\n",
    "CONSUMER_SECRET = 'Zm7B75DdwBDx5rLCMhLChQXhWEEfhqKf00gJFXYL7crFoDJi9A'\n",
    "\n",
    "text_list = ['racism', 'george floyd', 'police brutality', 'black lives matter', \n",
    "             'killing black', 'blacklivesmatter', 'police violence', 'ahmaud arbery'] \n",
    "hashtag_list = ['#blacklivesmatter', '#blm', '#JusticeForGeorgeFloyd',' #ICantBreathe', '#GeorgeFloyd']\n",
    "mentions_list = []\n",
    "from_list = []\n",
    "to_list = []\n",
    "\n",
    "full_list = text_list\n",
    "full_list.extend(hashtag_list)\n",
    "full_list.extend(mentions_list)\n",
    "full_list.extend(from_list)\n",
    "full_list.extend(to_list)\n",
    "\n",
    "credentials = Credentials(ACCESS_TOKEN, ACCESS_TOKEN_SECRET,\n",
    "    CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "file_name_base = 'data/raw/AMIR/' #enter path where you want the file saved to\n",
    "\n",
    "mine_tweets(credentials, full_list, file_name_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twarc filter blacklivesmatter,blm > rt_tweets.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twarc filter blacklivesmatter,blm > rt_tweets.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twarc search 'blacklivesmatter OR police OR violence OR #blacklivesmatter OR #blm' > past_tweets.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twarc search '#blacklivesmatter OR #blm' > past_tweets.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('rt_tweets.txt', sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('rt_tweets.jsonl') as tweet_data:\n",
    "    json_data = json.load(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('tweets_test2.jsonl', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
